==========
Args:Namespace(arch='resnet50tp', dataset='mars', eval_step=50, evaluate=False, gamma=0.1, gpu_devices='0', height=224, htri_only=False, lr=0.0003, margin=0.3, max_epoch=1, num_instances=4, pool='avg', pretrained_model='/home/jiyang/Workspace/Works/video-person-reid/3dconv-person-reid/pretrained_models/resnet-50-kinetics.pth', print_freq=80, save_dir='log', seed=1, seq_len=4, start_epoch=0, stepsize=200, test_batch=1, train_batch=1, use_cpu=False, weight_decay=0.0005, width=112, workers=4)
==========
Currently using GPU 0
Initializing dataset mars
=> MARS loaded
Dataset statistics:
  ------------------------------
  subset   | # ids | # tracklets
  ------------------------------
  train    |   625 |     8298
  query    |   626 |     1980
  gallery  |   622 |     9330
  ------------------------------
  total    |  1251 |    19608
  number of images per tracklet: 2 ~ 920, average 59.5
  ------------------------------
Initializing model: resnet50tp
Model size: 24.78866M
==> Epoch 1/1
Batch 80/2500	 Loss 6.901227 (6.501728)
Batch 160/2500	 Loss 6.623084 (6.689403)
Batch 240/2500	 Loss 6.893158 (6.756009)
Batch 320/2500	 Loss 6.857597 (6.800836)
Batch 400/2500	 Loss 6.653900 (6.809885)
Batch 480/2500	 Loss 6.470464 (6.766030)
Batch 560/2500	 Loss 6.509492 (6.728776)
Batch 640/2500	 Loss 6.507866 (6.698776)
Batch 720/2500	 Loss 6.499753 (6.675739)
Batch 800/2500	 Loss 6.482631 (6.656336)
Batch 880/2500	 Loss 6.522867 (6.641501)
Batch 960/2500	 Loss 6.510635 (6.630561)
Batch 1040/2500	 Loss 6.520986 (6.622203)
Batch 1120/2500	 Loss 6.509119 (6.615084)
Batch 1200/2500	 Loss 6.528092 (6.609161)
Batch 1280/2500	 Loss 6.502052 (6.604434)
Batch 1360/2500	 Loss 6.560069 (6.601864)
Batch 1440/2500	 Loss 6.555643 (6.599744)
Batch 1520/2500	 Loss 6.575097 (6.598661)
Batch 1600/2500	 Loss 6.561558 (6.598199)
Batch 1680/2500	 Loss 6.601358 (6.598039)
Batch 1760/2500	 Loss 6.549087 (6.598066)
Batch 1840/2500	 Loss 6.626961 (6.598833)
Batch 1920/2500	 Loss 6.610799 (6.600486)
Batch 2000/2500	 Loss 6.561647 (6.602291)
Batch 2080/2500	 Loss 6.612247 (6.604430)
Batch 2160/2500	 Loss 6.591648 (6.606838)
Batch 2240/2500	 Loss 6.689992 (6.609169)
Batch 2320/2500	 Loss 6.628504 (6.612752)
Batch 2400/2500	 Loss 6.672963 (6.616295)
Batch 2480/2500	 Loss 6.641853 (6.620255)
==> Test
